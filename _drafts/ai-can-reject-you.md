---
layout: post
title:  "AI Can Reject YOU!"
description: AI can literally reject you and there is nothing that you can do about it. And yes, it is going to affect you more than you think. 
author: Ahmad Alobaid
length: 3
categories: ["AI", "Ethics"]
cover: sample.png
toc: true
---



As the adoption of Artificial Intelligence (AI) increase and their benefits took the companies and organisation by a storm that some started using it without fully understanding what is going on (or basically ignoring their inner voice). This is not about having AI robots knocking on the door terminator-style. This is something actually happening, which snuck into the day-to-day activities.







# Is AI rejecting you? 
Have you once went for a bank to get a loan? they do [risk assessment](https://en.wikipedia.org/wiki/Credit_risk) before making a decision. Basically, they try to predict whether you will be able to pay the loan back plus the interest. Have you been to a bank to get a credit card and they asked for a deposit? most probably because they do not think that you will be able to pay it back.

<br>
Have you went to get an insurance and the price was ridiculously high? or worse, they even refused to provide you with one? just because you were born with some unfortunate health condition? [More about that here](https://www.reuters.com/article/business/healthcare-pharmaceuticals/genetic-conditions-often-lead-to-insurance-refusal-idUSCOL575585/)


<br>

How about applying for a job, and never getting a callback even though you are the perfect person for the position? how about crushing the interview and doing really well, but they decided to reject you? maybe they would hire you if the AI system didn't flagged you as "not recommended". This might not have anything to do with you being a good or a bad candidate. You might be interested what happened when [Amazon used AI](https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/)
 to help them in the hiring process by vetting resumes.

<!--
And even worse, they want you to join but the "system" flagged you as "not recommended"? This might not have anything to do with you being a good or a bad candidate, it is about the **why**. 
-->
<br>

Amazon used an AI hiring system, but it turned out to be problematic, it was "sexist" as described by the [BBC](https://www.bbc.com/news/technology-45809919). It preferred male candidates to female candidates and would penalize resumes that mention women's club. There is a scientific explanation for that and it is easy to understand. The AI system was most likely using historic data to learn and optimize for the best results.
 Below we explain how that can happen without actually meaning to be sexist (or biased).

 
# Why it is doing so?
<!-- 
# Can AI inherit our bias?
-->

Let us imagine that we provided the AI system with two piles of resumes: one has the good ones and another has the bad ones. The AI system would "study" those resumes and come up with some theories or ideas about what constitutes a good resume and what makes a resume bad (or unfit for the position). 

<br>


It is important to emphasize that whatever rules or knowledge the AI "learned" does not necessarily have to [make sense](https://towardsdatascience.com/irrational-ai-6f0b6d25af8f). For example, it may "learn" that resumes that uses *Arial* font is better (more fit to hire) that resumes written in *Times New Roman* (both are used in the [resume template](https://capd.mit.edu/blog/2023/09/01/enhance-your-resume-a-guide-for-first-year-undergraduates/) by MIT). We can agree that using one or the other has no relation whether the resume is good or bad. 


<br>

Having a large amount of data might solve part of this problem. In the same example, AI "learned" that *Arial* resumes are better than *Times New Roman*'s because the pile of accepted resumes has resumes written mostly in *Arial* while the rejected pile resumes are mostly written in *Times New Roman*. 
 It can also learn some more logical attributes, such as having relevant experience, needed skills, (required) education, fit for the position, [etc](https://www.alifeafterlayoff.com/7-things-a-recruiter-looks-for-on-a-resume/). 


<br>

But, what if hiring managers are preferring graduates from ivy league universities [[1](https://www.fastcompany.com/3060544/despite-evidence-hiring-managers-are-biased-towards-graduates-from-top-colleges),[2](https://bigthink.com/business/case-against-hiring-ivy-league-schools/),[3](https://news.ycombinator.com/item?id=37676555),[4](https://www.wsj.com/articles/why-i-stopped-hiring-ivy-league-graduates-11623103004) ], or people from certain neighborhoods [[5](https://hbr.org/2018/12/research-hiring-managers-are-biased-against-people-with-longer-commutes), [6](https://www.cairn-int.info/article-E_RECO_673_0525--effects-of-local-neighbourhood-effects.htm) ]  or people from a specific background or race [[7](https://www.inc.com/bruce-crumley/heres-exactly-where-racial-bias-in-hiring-persists-and-how-businesses-can-address-it.html), [8](https://news.northwestern.edu/stories/2023/01/racial-discrimination-in-hiring-remains-a-persistent-problem-northwestern-study?fj=1) ] , then the AI system would pick up on that. This does not mean that these hiring managers intended to be bias; it can be an [unconscious bias](https://www.hbs.edu/recruiting/insights-and-advice/blog/post/actively-addressing-unconscious-bias-in-recruiting). Whether biases backed by analytics is ethical or not is another question related to ethics. 


<br>

 There are [ways](https://hbr.org/2017/06/7-practical-ways-to-reduce-bias-in-your-hiring-process) to reduce bias such as [standardize interviews](https://hbr.org/2016/04/how-to-take-the-bias-out-of-interviews?utm_campaign=harvardbiz&utm_source=twitter&utm_medium=social) and objectively defining [culture fit](https://hbr.org/2019/11/how-the-best-bosses-interrupt-bias-on-their-teams?utm_medium=paidsearch&utm_source=google&utm_campaign=intlcontent_bussoc&utm_term=Non-Brand&tpcc=intlcontent_bussoc&gad_source=1&gclid=EAIaIQobChMI6N7g-KSwhwMVk2hBAh3gcwawEAMYASAAEgILhfD_BwE). But for an AI system, it does not care about the implemented measures to combate bias, it only react to the data it is being fed. If the implemented measures reduced the bias, hence the data will have less bias, and the AI will inherently be less biased. There are also [other techniques](https://www.holisticai.com/blog/bias-mitigation-strategies-techniques-for-classification-tasks) to compate bias that can be implemented as well.

<br>



# How to protect yourself?
**Awareness**. According to a survey done by [paw research](https://www.pewresearch.org/science/2023/02/15/public-awareness-of-artificial-intelligence-in-everyday-activities/), around 44% of people think they do not regularly interact with AI. Being aware of the employement of such systems and how they are being used might help us eliminate or reduce the possible negative outcomes. There is no one easy magical thing that people can do to protect themselves against AI as individuals (at least, practically).

<br>

 But awareness can be helpful on a case-by-case basis. People can change their behaviour to maximize the likelihood of a positive outcome. So if we are talking about the hiring process, people can do certain things to be more "hireable" for an AI system. Using certain (or the "correct") [keywords](https://www.alifeafterlayoff.com/keywords-on-your-resume-dont-work/) might improve your chances. Some people also tried gaming such systems by using a technique called *keyword stuffing*, which is adding keywords to the resume to trick the system into ranking your resume higher. [Linkedin](https://www.linkedin.com/advice/0/what-risks-keyword-stuffing-your-resume-skills-resume-writing-fzlzc#:~:text=Keyword%20stuffing%20is%20the%20practice,experience%20or%20the%20job%20description.) and [others](https://www.alifeafterlayoff.com/keywords-on-your-resume-dont-work/) claim that it also have a negative effect. 
  Regardless, being aware of such techniques and how AI is reacting to them can empower us to make the best out of this situation.  

  <br>

Researchers in the AI domain can also develop techniques to help protect people's privacy. Researchers from Australia and the United States developed a way to trick image recognition systems called [TnT Attacks](https://doi.org/10.1109/TIFS.2022.3198857). You can see a video of how they tricked the AI system by showing it a flower which the recognition system detected it as Barak Obama. A demo can be found [here](https://tntattacks.github.io/). An Italian fashion startup called [Capable](https://www.capable.design/) sells cloths with prints to confuse image recognition software as reported by the [CNN](https://edition.cnn.com/2023/01/16/tech/facial-recognition-fashion/index.html). Researchers at McAfee tricked Tesla into speeding up using tape [here](https://www.wired.com/story/tesla-speed-up-adversarial-example-mgm-breach-ransomware/). However, people have to be aware as some actions by be unethical or even illegal. 
 
 

# How organizations might do it?
1. **Technical Bias Mitigation Techniques**. There are certain measures that can be used by organisations to reduce the negative effects of AI use. These are [technical measures](https://www.holisticai.com/blog/bias-mitigation-strategies-techniques-for-classification-tasks) which can be employed to reduce the possible [bias](https://pubmed.ncbi.nlm.nih.gov/37845518/) (e.g., [resampling](https://www.statisticssolutions.com/dissertation-resources/sample-size-calculation-and-sample-size-justification/resampling/#:~:text=Resampling%20involves%20the%20selection%20of,to%20the%20original%20data%20sample.), [reweighting](https://web.stanford.edu/~lexing/resw.pdf)).
2. **Transparent Protocol**. If you ask an organization which is using AI about the protocol they are using, they probably would not share it. It is important for organizations to employ a way to ask for the AI decision to be reviewed by a human. However, it might be hard to make it work in practice, as employees can also insist on always picking the same decision suggested by the AI system. But as a start, having a transparent protocol on how such cases will be handeled might mitigate the negative outcomes. You can also check how the EU have different requirements depending on the risk, such as the [EU AI Act](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence#transparency-requirements-1) and the [EU Regulatory Framework on AI](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai). 
3. **Review Cycle**. Having humans in the loop is very crucial. Continously reviewing what is [going on](https://www.charthop.com/resources/combat-workplace-bias-with-performance-reviews) and being aware of the different [kinds of biases](https://www.cultureamp.com/blog/performance-review-bias), especially the ones comitted by humans can shed the light into what to look for in these reviews.

















<!-- 
1. **Awareness**. According to a survey done by [paw research](https://www.pewresearch.org/science/2023/02/15/public-awareness-of-artificial-intelligence-in-everyday-activities/), around 44% of people think they do not regularly interact with AI. Being aware of the employement of such systems and how they are being used might help us eliminate or reduce the possible negative outcomes. There is no one easy magical thing that people can do to protect themselves against AI as individuals (at least, practically). But awareness can be helpful on a case-by-case basis. People can change their behaviour to maximize the likelihood of a positive outcome. So if we are talking about the hiring process, people can do certain things to be more "hireable" for an AI system. Using certain (or the "correct") [keywords](https://www.alifeafterlayoff.com/keywords-on-your-resume-dont-work/) might improve your chances. Some people also use a technique called *keyword stuffing*, which is adding keywords to the resume to trick the system into ranking your resume higher. [Linkedin](https://www.linkedin.com/advice/0/what-risks-keyword-stuffing-your-resume-skills-resume-writing-fzlzc#:~:text=Keyword%20stuffing%20is%20the%20practice,experience%20or%20the%20job%20description.) and [others](https://www.alifeafterlayoff.com/keywords-on-your-resume-dont-work/) claim that it also have a negative effect. 
Regardless of the details, being aware of such techniques and how AI is reacting to them can empower us to make the best out of this situation. 
2. **Transparent Protocol**. You can try to ask whether they have any protocols. Probably, they won't disclose them. It is important for organizations to employ a way to ask for the AI decision to be reviewed by a human or by another AI system. This is very crucial, but it might be hard to make it work in practice, as employees can also insist on always picking the same decision suggested by the AI system. But as a start, having a transparent protocol on how such cases will be handeled might mitigate the negative outcomes. You can check [EU AI Act](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence#transparency-requirements-1), (note that the transparent protocol is not yet implemented).
3. **AI trickery**. By knowing what and how the AI systems are performing the different tasks, one might exploit that to their advantage. This can range from including certain keywords preferred by the AI systems to lying about the resume. Some researchers even developed a way to trick image recognition systems called [TnT Attacks](https://arxiv.org/abs/2111.09999). You can see a video of how their tricked the AI system by showing it a flower which the recognition system detected it as Barak Obama. A demo can be found [here](https://tntattacks.github.io/). You can also see how some people tricked Tesla into speeding up using tape [here](https://www.wired.com/story/tesla-speed-up-adversarial-example-mgm-breach-ransomware/). Note that some of the activities of AI trickery can be unethical or even illegal. 




knowledge about the AI use can empower us. 
But this can also be used for banking systems like asking for a credit cards and loans. Knowing the metrics can be helpful and you can increase your chance. To what extend this is ethical is another story.

 This does not only work for the hiring process, but also in the daily activities. 
 
 But how being aware can actually help you? knowing the use of such tools can help you change your actions to be more "hireable" if we are talking about the job market. Some people even tried using AI for their advantage by using similar systems and testing their resume. But this can also be used for banking systems like asking for a credit cards and loans. Knowing the metrics can be helpful and you can increase your chance. To what extend this is ethical is another story.

  If you know that the company is using an AI powered system for selecting candidates which checks for certain keywords or achievements, maybe you can emphasize that in your resume (even though this must be there anyways).
  
a resume a good or bad one. It can learn thi

  basically it learned the following: this pile consists of good resumes and this pile consists of bad resumes.
The data provided probably rejected the ones with all women colleges or the ones which the word women in it. It the AI itself was not sexist, it optimized based on the provided data. 



 One of the signals that would result in a lower score is having a mention of a women club in the resume. []

 as it ranked candidates with relatino to female clubs lower


 predicted if a candidate has something related to a female club, it would negatively affect their score -- the probability to get hired). You can read about this story on reuters [here](https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/).
 [](https://www.bbc.com/news/technology-45809919)


It is hard to know for sure whether you have been reject by AI, the humans in charge, or a mixture of both. Some actually advertise that they are using AI for 
certain tasks, others might not disclouse that information. But how does an AI system reject someone? how they make such decisions?


 Would a hiring
 Now we understand how this might happen. If you are interested about the ivy league hiring you can look at the following resources: [here](https://www.fastcompany.com/3060544/despite-evidence-hiring-managers-are-biased-towards-graduates-from-top-colleges), [here](https://bigthink.com/business/case-against-hiring-ivy-league-schools/), [here](https://news.ycombinator.com/item?id=37676555), and [here](https://www.wsj.com/articles/why-i-stopped-hiring-ivy-league-graduates-11623103004).



# But why it is happening to us?




# Why does AI make these decision?

Because it was told to do so. The way such systems are often used is to perform a task and optimizing for some outcome. 

## Why they use AI
AI is attrative to organizations for the following benefits: automation, performance and cost. It might not always be that is it used for all of them, but at least two fo them would make it worth considering at least. Automating certain tasks can reduce the cost of the resulted outcome is good enough. Even if the performance of AI systems is not very high, it can be helpful if paired with experts in the field. 


## AI as an employee

Because it is the way AI is being used. This is not a problem with the AI per se; the same as the problem of wars is not because of the invention of knives or guns. 
AI can be used as a black box or as a white box. 

### AI as a black box
AI can be optimized for a certain outcome. It follows the philosophy "the end justifies the means". You do not necessarily tell it "how to" achieve the goal, you just tell it to get the best results. So if applied on the sports domain, it try to score the highest number of goals, regardless of the number of "violations", or "fouls" (even though it might learn to avoid them at certain times). 

When AI is applied to the hiring process, it would focus on getting the highest possible success rate, even if that mean it can be racist, sexist, and other *ists. Even if you tell it not do any of that, it might not really follow through as it might not really understand the concepts as we human do. 


### AI as a white box
This kind is AI is not about being good and sane. It just means that it can easily be traced. The AI is being told "how to" optimise. Using the sport metaphore, it would mean to score goals when it can. Try to run in a certain way and avoid giving the opponent the opportunity to score. 

When applied for hiring, it would mean to look for the education, the years of experience, accomplishments, and how they are aligned with the job description. This does not mean that it is fair, it only means that it follows what it is told to do without follwing unknown path to acheive the goal as the case of black box.


It can learn, for example, that resumes that have the name "John" are good while resumes which have the name "Bill" are bad. Or it can learn that resumes of people born in summer are good while resumes of people born in winter are bad. 

There are also some resources which discuss bias related to education, such as hiring from ivy league: [1](https://www.fastcompany.com/3060544/despite-evidence-hiring-managers-are-biased-towards-graduates-from-top-colleges), [2](https://bigthink.com/business/case-against-hiring-ivy-league-schools/), [3](https://news.ycombinator.com/item?id=37676555), and [4](https://www.wsj.com/articles/why-i-stopped-hiring-ivy-league-graduates-11623103004). 


such as having a good GPA, being in an ivy league university, having a masters, winning some competitions, having more years of experience, having personal projects, and so on. Now, you might argue that these are not necessarily what makes a resume good, or a prospective employee a good fit, but we can understand why.


Regardless, being aware of such techniques and how AI is reacting to them can empower us to make the best out of this situation.  

By knowing what and how the AI systems are performing the different tasks, one might exploit that to their advantage. 

Some researchers even developed a way to trick image recognition systems called [TnT Attacks](https://doi.org/10.1109/TIFS.2022.3198857). You can see a video of how their tricked the AI system by showing it a flower which the recognition system detected it as Barak Obama. A demo can be found [here](https://tntattacks.github.io/). You can also see how some people tricked Tesla into speeding up using tape [here](https://www.wired.com/story/tesla-speed-up-adversarial-example-mgm-breach-ransomware/). Note that some of the activities of AI trickery can be unethical or even illegal. 

Regardless, being aware of such techniques and how AI is reacting to them can empower us to make the best out of this situation.  


Have you went to get an insurance and the price was ridiculously high? or worse, they even refused to provide you with one? either because your friend crashed the car, or you were born with some unfortunate health condition?


--> 





